{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "db2b9359-afde-4052-b741-e3673ab14579",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3e1b9651-ca21-474e-b52c-ce11661aedcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arghasreebanerjee/opt/anaconda3/envs/XAI_week1/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "n_state = env.observation_space.shape[0] \n",
    "n_action = env.action_space.n\n",
    "n_episode = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7dd15304-c509-48ba-8377-5c99b4ec5990",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, weight): \n",
    "    state = env.reset()[0]  # initial state\n",
    "    total_reward = 0\n",
    "    is_done = False\n",
    "    while not is_done:\n",
    "        state = torch.from_numpy(state).float()\n",
    "        multiply = torch.matmul(state, weight)\n",
    "        # print(f'State ({state.size()}) X Weight ({weight.size()}) = {multiply.size()}')\n",
    "        # print('Choosing action based on the weights of invidual actions ... ', multiply,'\\n')\n",
    "        action = torch.argmax(multiply) # previous line \n",
    "        state, reward, is_done, _, info = env.step(action.item()) # Instead of sampling the action from a uniform distribution directly, we assign weights and then sample in previous line\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e74b633-a841-493c-8f1f-c3b83ec98eb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "best_total_reward = 0\n",
    "best_weight = torch.rand(n_state, n_action)\n",
    "noise_scale = 0.01 # Hill- climbing works by taking increasing the weight by some random noise, if the rewards increase in 1 episode, change the weights, or keep it the same. \n",
    "# the noise_scale is a scaling factor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f19d172f-f4f8-48b6-baae-56692611627f",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Rewards 275.0\n",
      "Episode 2: Rewards 2941.0\n",
      "Episode 3: Rewards 419.0\n",
      "Episode 4: Rewards 1697.0\n",
      "Episode 5: Rewards 2008.0\n",
      "Episode 6: Rewards 872.0\n",
      "Episode 7: Rewards 703.0\n",
      "Episode 8: Rewards 405.0\n",
      "Episode 9: Rewards 5826.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_episode):\n\u001b[1;32m      3\u001b[0m     weight \u001b[38;5;241m=\u001b[39m best_weight\u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(n_state, n_action)\n\u001b[0;32m----> 4\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;241m>\u001b[39m best_total_reward:\n\u001b[1;32m      6\u001b[0m         best_total_reward \u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[4], line 7\u001b[0m, in \u001b[0;36mrun_episode\u001b[0;34m(env, weight)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_done:\n\u001b[1;32m      6\u001b[0m     state \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(state)\u001b[38;5;241m.\u001b[39mfloat()\n\u001b[0;32m----> 7\u001b[0m     multiply \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# print(f'State ({state.size()}) X Weight ({weight.size()}) = {multiply.size()}')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print('Choosing action based on the weights of invidual actions ... ', multiply,'\\n')\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(multiply) \u001b[38;5;66;03m# previous line \u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "total_rewards=[]\n",
    "for episode in range(n_episode):\n",
    "    weight = best_weight+ torch.rand(n_state, n_action)\n",
    "    reward = run_episode(env, weight)\n",
    "    if reward > best_total_reward:\n",
    "        best_total_reward = reward\n",
    "        best_weight = weight\n",
    "    total_rewards.append(reward)\n",
    "    print(f'Episode {episode+1}: Rewards {reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "27e8496b-2e05-4e36-bebb-549308d03576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards = 112.81\n"
     ]
    }
   ],
   "source": [
    "print(f'Average rewards = {sum(total_rewards)/n_episode}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6117c3a9-0005-46c3-a4fe-18acfad66c41",
   "metadata": {},
   "source": [
    "If in the next episode, the rewards have increased, then it is a good idea to walk away from the current weight values. So decrease the scalling factor. \\\n",
    "If not, then don't add so much noise.\n",
    "\n",
    "Things to worry about:\n",
    "- How much should the noise increase or decrease? \n",
    "- Set a lower/upper bound so that the noise scale is not too low. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "6ee271f7-0e97-40e2-92d7-8df907487c12",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1: Rewards 9.0\n",
      "Episode 2: Rewards 26.0\n",
      "Episode 3: Rewards 10.0\n",
      "Episode 4: Rewards 24.0\n",
      "Episode 5: Rewards 17.0\n",
      "Episode 6: Rewards 45.0\n",
      "Episode 7: Rewards 57.0\n",
      "Episode 8: Rewards 46.0\n",
      "Episode 9: Rewards 44.0\n",
      "Episode 10: Rewards 78.0\n",
      "Episode 11: Rewards 57.0\n",
      "Episode 12: Rewards 69.0\n",
      "Episode 13: Rewards 103.0\n",
      "Episode 14: Rewards 111.0\n",
      "Episode 15: Rewards 513.0\n",
      "Episode 16: Rewards 46.0\n",
      "Episode 17: Rewards 41.0\n",
      "Episode 18: Rewards 110.0\n",
      "Episode 19: Rewards 46.0\n",
      "Episode 20: Rewards 36.0\n",
      "Episode 21: Rewards 120.0\n",
      "Episode 22: Rewards 163.0\n",
      "Episode 23: Rewards 122.0\n",
      "Episode 24: Rewards 59.0\n",
      "Episode 25: Rewards 36.0\n",
      "Episode 26: Rewards 89.0\n",
      "Episode 27: Rewards 167.0\n",
      "Episode 28: Rewards 42.0\n",
      "Episode 29: Rewards 55.0\n",
      "Episode 30: Rewards 170.0\n",
      "Episode 31: Rewards 200.0\n",
      "Episode 32: Rewards 96.0\n",
      "Episode 33: Rewards 55.0\n",
      "Episode 34: Rewards 84.0\n",
      "Episode 35: Rewards 121.0\n",
      "Episode 36: Rewards 63.0\n",
      "Episode 37: Rewards 242.0\n",
      "Episode 38: Rewards 56.0\n",
      "Episode 39: Rewards 264.0\n",
      "Episode 40: Rewards 44.0\n",
      "Episode 41: Rewards 45.0\n",
      "Episode 42: Rewards 409.0\n",
      "Episode 43: Rewards 91.0\n",
      "Episode 44: Rewards 334.0\n",
      "Episode 45: Rewards 316.0\n",
      "Episode 46: Rewards 61.0\n",
      "Episode 47: Rewards 60.0\n",
      "Episode 48: Rewards 230.0\n",
      "Episode 49: Rewards 60.0\n",
      "Episode 50: Rewards 72.0\n",
      "Episode 51: Rewards 63.0\n",
      "Episode 52: Rewards 103.0\n",
      "Episode 53: Rewards 253.0\n",
      "Episode 54: Rewards 51.0\n",
      "Episode 55: Rewards 55.0\n",
      "Episode 56: Rewards 105.0\n",
      "Episode 57: Rewards 160.0\n",
      "Episode 58: Rewards 203.0\n",
      "Episode 59: Rewards 95.0\n",
      "Episode 60: Rewards 94.0\n",
      "Episode 61: Rewards 156.0\n",
      "Episode 62: Rewards 205.0\n",
      "Episode 63: Rewards 58.0\n",
      "Episode 64: Rewards 59.0\n",
      "Episode 65: Rewards 194.0\n",
      "Episode 66: Rewards 171.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m episode \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_episode):\n\u001b[1;32m      8\u001b[0m     weight \u001b[38;5;241m=\u001b[39m best_weight \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mrand(n_state, n_action)\n\u001b[0;32m----> 9\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[43mrun_episode\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m reward \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m best_total_reward:\n\u001b[1;32m     11\u001b[0m         best_total_reward \u001b[38;5;241m=\u001b[39m reward\n",
      "Cell \u001b[0;32mIn[4], line 11\u001b[0m, in \u001b[0;36mrun_episode\u001b[0;34m(env, weight)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;66;03m# print(f'State ({state.size()}) X Weight ({weight.size()}) = {multiply.size()}')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;66;03m# print('Choosing action based on the weights of invidual actions ... ', multiply,'\\n')\u001b[39;00m\n\u001b[1;32m     10\u001b[0m     action \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39margmax(multiply) \u001b[38;5;66;03m# previous line \u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m     state, reward, is_done, _, info \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Instead of sampling the action from a uniform distribution directly, we assign weights and then sample in previous line\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     total_reward \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m reward\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_reward\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/XAI_week1/lib/python3.10/site-packages/gymnasium/wrappers/time_limit.py:57\u001b[0m, in \u001b[0;36mTimeLimit.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep\u001b[39m(\u001b[38;5;28mself\u001b[39m, action):\n\u001b[1;32m     47\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Steps through the environment and if the number of steps elapsed exceeds ``max_episode_steps`` then truncate.\u001b[39;00m\n\u001b[1;32m     48\u001b[0m \n\u001b[1;32m     49\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     55\u001b[0m \n\u001b[1;32m     56\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     58\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_elapsed_steps \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_max_episode_steps:\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/XAI_week1/lib/python3.10/site-packages/gymnasium/wrappers/order_enforcing.py:56\u001b[0m, in \u001b[0;36mOrderEnforcing.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_reset:\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m ResetNeeded(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot call env.step() before calling env.reset()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 56\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/XAI_week1/lib/python3.10/site-packages/gymnasium/wrappers/env_checker.py:51\u001b[0m, in \u001b[0;36mPassiveEnvChecker.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m env_step_passive_checker(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv, action)\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 51\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/XAI_week1/lib/python3.10/site-packages/gymnasium/envs/classic_control/cartpole.py:165\u001b[0m, in \u001b[0;36mCartPoleEnv.step\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    161\u001b[0m     theta \u001b[38;5;241m=\u001b[39m theta \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtau \u001b[38;5;241m*\u001b[39m theta_dot\n\u001b[1;32m    163\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate \u001b[38;5;241m=\u001b[39m (x, x_dot, theta, theta_dot)\n\u001b[0;32m--> 165\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mbool\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    166\u001b[0m \u001b[43m    \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_threshold\u001b[49m\n\u001b[1;32m    167\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx_threshold\u001b[49m\n\u001b[1;32m    168\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m<\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta_threshold_radians\u001b[49m\n\u001b[1;32m    169\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtheta\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtheta_threshold_radians\u001b[49m\n\u001b[1;32m    170\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminated:\n\u001b[1;32m    173\u001b[0m     reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1.0\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_total_reward = 0\n",
    "best_weight = torch.rand(n_state, n_action)\n",
    "noise_scale = 0.01 \n",
    "total_rewards=[]\n",
    "n_episode = 200\n",
    "\n",
    "for episode in range(n_episode):\n",
    "    weight = best_weight + torch.rand(n_state, n_action)\n",
    "    reward = run_episode(env, weight)\n",
    "    if reward >= best_total_reward:\n",
    "        best_total_reward = reward\n",
    "        best_weight = weight\n",
    "        noise_scale = max(noise_scale/2, 1e-4)\n",
    "    else:\n",
    "        noise_scale = min(noise_scale*2, 2)\n",
    "    total_rewards.append(reward)\n",
    "    print(f'Episode {episode+1}: Rewards {reward}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2e27a0b3-507c-4852-8134-8ad90edea20d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average rewards = 37.195\n"
     ]
    }
   ],
   "source": [
    "print(f'Average rewards = {sum(total_rewards)/n_episode}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792ac70e-5da2-4d06-bb35-613947de3cb1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
