{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "680451c7-9ba4-4734-ba29-529e7e7da8bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16\n",
      "4\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "import torch\n",
    "env = gym.make(\"FrozenLake-v1\", render_mode=\"human\")\n",
    "n_state = env.observation_space.n\n",
    "print(n_state)\n",
    "n_action = env.action_space.n\n",
    "print(n_action)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "774550a4-2a24-4d20-a7af-fa8770ce59b1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0, {'prob': 1})"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0ea391a2-72ff-4df7-8ecc-497f2923b747",
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render() # renders the environment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7f2a89c6-eb25-4f93-abe4-86c530bd301f",
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state, reward, is_done, truncated, info = env.step(1)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "a07baab3-c8ee-466c-9b4f-540f57eda57a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0.0\n",
      "False\n",
      "{'prob': 0.3333333333333333}\n"
     ]
    }
   ],
   "source": [
    "print(new_state)\n",
    "print(reward)\n",
    "print(is_done)\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "8db1e3b3-1488-4119-9eac-e2734a0392b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, policy):\n",
    "    state = env.reset()[0]\n",
    "    total_reward = 0\n",
    "    is_done = False\n",
    "    while not is_done:\n",
    "        action = policy[state].item()\n",
    "        state, reward, is_done, trunc, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        if is_done:\n",
    "            break\n",
    "    return total_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ec77f3ee-0c3c-452a-9fb0-cc7b94fed170",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward under random policy: 0.0\n"
     ]
    }
   ],
   "source": [
    "n_episode = 10\n",
    "total_rewards = []\n",
    "for episode in range(n_episode):\n",
    "    random_policy = torch.randint(high=n_action, size=(n_state,))\n",
    "    # print(f'The random policy is {random_policy}')\n",
    "    total_reward = run_episode(env, random_policy)  # In every episode, a new policy is sampled, and the episode runs with a discrete action associated with each state. \n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "print('Average total reward under random policy: {}'.format(sum(total_rewards) / n_episode))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5ff32aa2-dc5c-4a7f-a475-8d4a86dc571f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 .. 1 .. 2 .. 3 .. 4 .. 5 .. 6 .. 7 .. 8 .. 9 .. 10 .. 11 .. 12 .. 13 .. 14 .. 15 .. 16 .. 17 .. 18 .. 19 .. 20 .. 21 .. 22 .. 23 .. 24 .. 25 .. 26 .. 27 .. 28 .. 29 .. 30 .. 31 .. 32 .. 33 .. 34 .. 35 .. 36 .. 37 .. 38 .. 39 .. 40 .. 41 .. 42 .. 43 .. 44 .. 45 .. 46 .. "
     ]
    }
   ],
   "source": [
    "i=0\n",
    "while True:\n",
    "    print(f\"{i} .. \",end = '')\n",
    "    random_policy = torch.randint(high=n_action, size=(n_state,))\n",
    "    total_reward = run_episode(env, random_policy)\n",
    "    if total_reward == 1:\n",
    "        best_policy = random_policy\n",
    "        break\n",
    "    i+=1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88976b1c-14a2-4a8e-80cd-0bb362570c6c",
   "metadata": {},
   "source": [
    "Even though the policy was successful to reach the goal in the 46 th episode, that does not mean it will always result in success. \\\n",
    "For example, see below. \\\n",
    "The reason this happens is that even though the action is taken deterministically, the same action might not end up been taken, because of slippery ice. \\\n",
    "Direct quote - \"in FrozenLake, the movement direction is only partially dependent on the chosen action.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1338766b-4f6d-4413-9a7a-1bc85bd3a294",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average total reward under random search policy: 0.0\n"
     ]
    }
   ],
   "source": [
    "total_rewards = []\n",
    "for episode in range(n_episode):\n",
    "    total_reward = run_episode(env, best_policy)\n",
    "    total_rewards.append(total_reward)\n",
    "\n",
    "print('Average total reward under random search policy: {}'.format(sum(total_rewards) / n_episode))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "94b337a7-a7fe-4c63-962a-1260a6fbfab0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)], 1: [(0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True)], 2: [(0.3333333333333333, 10, 0.0, False), (0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False)], 3: [(0.3333333333333333, 7, 0.0, True), (0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True)]}\n"
     ]
    }
   ],
   "source": [
    "print(env.env.P[6])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa69d9c5-ad9c-4804-8257-628059ae6661",
   "metadata": {},
   "source": [
    "Key : [0,1,2,3,4] \\\n",
    "Value :  a list of movements after taking an action. \\\n",
    "For eg. 0 : [(0.3333333333333333, 2, 0.0, False), (0.3333333333333333, 5, 0.0, True), (0.3333333333333333, 10, 0.0, False)] \\\n",
    "    (transformation probability), (new state) (rewards), (is done) \\\n",
    "This means if action 0 is chosen by our algorithm, the system chooses 0 with 1/3 probability. If state 0 is chosen, then the next step is 2, etc. Also, this happens only if the agent is in state '6'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9d714b24-bd14-471d-9f02-bc48331c83d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: [(1.0, 5, 0, True)], 1: [(1.0, 5, 0, True)], 2: [(1.0, 5, 0, True)], 3: [(1.0, 5, 0, True)]}\n"
     ]
    }
   ],
   "source": [
    "print(env.env.P[5]) # vs in state 5, when our algo selects 0, it takes that action with 100% certainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ae90f3a-164f-4fa0-b5c5-d58dc67b46f8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
