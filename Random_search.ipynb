{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ef10d3-e9bd-44cb-969d-33fa63a7d4e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "689ee09e-74b9-498d-b526-94c7af17481a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/arghasreebanerjee/opt/anaconda3/envs/XAI_week1/lib/python3.10/site-packages/gymnasium/envs/registration.py:513: DeprecationWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1`.\u001b[0m\n",
      "  logger.deprecation(\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "720bb85a-7fa3-4eda-974f-693eb370a2d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of elements to represent a state: 4\n",
      "The number of actions that the agent can take: 2\n"
     ]
    }
   ],
   "source": [
    "n_state = env.observation_space.shape[0] \n",
    "print(f'The number of elements to represent a state: {n_state}')\n",
    "n_action = env.action_space.n\n",
    "print(f'The number of actions that the agent can take: {n_action}')\n",
    "\n",
    "# So the agent can take any of the 2 actions from any of the 4 states.\n",
    "# Each of those transistions will have a weight \n",
    "# Total number of such weights = 8 (1 state has 2 weights, so 4 states have 8 weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cf4f996-f4ce-485f-af4e-11a9942f9ab1",
   "metadata": {},
   "source": [
    "### Each component of the state contributes differently to a certain action. For eg, action 1 = w1\\*component1 + w2*component2 \\* ...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "94bfbedb-272e-44f0-a219-c7b1cb136a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_episode(env, weight): \n",
    "    state = env.reset()[0]  # initial state\n",
    "    total_reward = 0\n",
    "    is_done = False\n",
    "    while not is_done:\n",
    "        state = torch.from_numpy(state).float()\n",
    "        multiply = torch.matmul(state, weight)\n",
    "        # print(f'State ({state.size()}) X Weight ({weight.size()}) = {multiply.size()}')\n",
    "        # print('Choosing action based on the weights of invidual actions ... ', multiply,'\\n')\n",
    "        action = torch.argmax(multiply) # previous line \n",
    "        state, reward, is_done, _, info = env.step(action.item()) # Instead of sampling the action from a uniform distribution directly, we assign weights and then sample in previous line\n",
    "        total_reward += reward\n",
    "    return total_reward\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a39e1a6-3954-4590-a17e-c0260fd5f46a",
   "metadata": {},
   "source": [
    "What is an episode? \n",
    "\n",
    "- The agent plays the game till the game is over. \n",
    "- The agent plays with the given weight distribution among actions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "63f96e74-a263-4d0f-b95b-d8a26793c3ae",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State (torch.Size([4])) X Weight (torch.Size([4, 2])) = torch.Size([2])\n",
      "Choosing action based on the weights of invidual actions ...  tensor([-0.0154, -0.0543]) \n",
      "\n",
      "State (torch.Size([4])) X Weight (torch.Size([4, 2])) = torch.Size([2])\n",
      "Choosing action based on the weights of invidual actions ...  tensor([ 0.1079, -0.0492]) \n",
      "\n",
      "State (torch.Size([4])) X Weight (torch.Size([4, 2])) = torch.Size([2])\n",
      "Choosing action based on the weights of invidual actions ...  tensor([ 0.2279, -0.0441]) \n",
      "\n",
      "State (torch.Size([4])) X Weight (torch.Size([4, 2])) = torch.Size([2])\n",
      "Choosing action based on the weights of invidual actions ...  tensor([ 0.3457, -0.0380]) \n",
      "\n",
      "State (torch.Size([4])) X Weight (torch.Size([4, 2])) = torch.Size([2])\n",
      "Choosing action based on the weights of invidual actions ...  tensor([ 0.4622, -0.0297]) \n",
      "\n",
      "State (torch.Size([4])) X Weight (torch.Size([4, 2])) = torch.Size([2])\n",
      "Choosing action based on the weights of invidual actions ...  tensor([ 0.5784, -0.0182]) \n",
      "\n",
      "State (torch.Size([4])) X Weight (torch.Size([4, 2])) = torch.Size([2])\n",
      "Choosing action based on the weights of invidual actions ...  tensor([ 0.6951, -0.0024]) \n",
      "\n",
      "State (torch.Size([4])) X Weight (torch.Size([4, 2])) = torch.Size([2])\n",
      "Choosing action based on the weights of invidual actions ...  tensor([0.8130, 0.0187]) \n",
      "\n",
      "State (torch.Size([4])) X Weight (torch.Size([4, 2])) = torch.Size([2])\n",
      "Choosing action based on the weights of invidual actions ...  tensor([0.9328, 0.0461]) \n",
      "\n",
      "State (torch.Size([4])) X Weight (torch.Size([4, 2])) = torch.Size([2])\n",
      "Choosing action based on the weights of invidual actions ...  tensor([1.0552, 0.0807]) \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "10.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weight = torch.rand(n_state, n_action)\n",
    "run_episode(env, weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "c46b35a4-d022-45b7-8af9-fdd248c74c2b",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1, Rewards : 12.0\n",
      "\n",
      "BETTER WEIGHTS FOUND !!!\n",
      "\n",
      "Episode 2, Rewards : 10.0\n",
      "Episode 3, Rewards : 10.0\n",
      "Episode 4, Rewards : 10.0\n",
      "Episode 5, Rewards : 11.0\n",
      "Episode 6, Rewards : 9.0\n",
      "Episode 7, Rewards : 15.0\n",
      "\n",
      "BETTER WEIGHTS FOUND !!!\n",
      "\n",
      "Episode 8, Rewards : 196.0\n",
      "\n",
      "BETTER WEIGHTS FOUND !!!\n",
      "\n",
      "Episode 9, Rewards : 50.0\n",
      "Episode 10, Rewards : 10.0\n",
      "Episode 11, Rewards : 9.0\n",
      "Episode 12, Rewards : 40.0\n",
      "Episode 13, Rewards : 11.0\n",
      "Episode 14, Rewards : 9.0\n",
      "Episode 15, Rewards : 10.0\n",
      "Episode 16, Rewards : 177.0\n",
      "Episode 17, Rewards : 158.0\n",
      "Episode 18, Rewards : 289.0\n",
      "\n",
      "BETTER WEIGHTS FOUND !!!\n",
      "\n",
      "Episode 19, Rewards : 9.0\n",
      "Episode 20, Rewards : 65.0\n",
      "Episode 21, Rewards : 23.0\n",
      "Episode 22, Rewards : 10.0\n",
      "Episode 23, Rewards : 43.0\n",
      "Episode 24, Rewards : 9.0\n",
      "Episode 25, Rewards : 72.0\n",
      "Episode 26, Rewards : 33.0\n",
      "Episode 27, Rewards : 9.0\n",
      "Episode 28, Rewards : 9.0\n",
      "Episode 29, Rewards : 83.0\n",
      "Episode 30, Rewards : 10.0\n",
      "Episode 31, Rewards : 9.0\n",
      "Episode 32, Rewards : 9.0\n",
      "Episode 33, Rewards : 456.0\n",
      "\n",
      "BETTER WEIGHTS FOUND !!!\n",
      "\n",
      "Episode 34, Rewards : 91.0\n",
      "Episode 35, Rewards : 28.0\n",
      "Episode 36, Rewards : 305.0\n",
      "Episode 37, Rewards : 8.0\n",
      "Episode 38, Rewards : 9.0\n",
      "Episode 39, Rewards : 10.0\n",
      "Episode 40, Rewards : 89.0\n",
      "Episode 41, Rewards : 53.0\n",
      "Episode 42, Rewards : 95.0\n",
      "Episode 43, Rewards : 165.0\n",
      "Episode 44, Rewards : 92.0\n",
      "Episode 45, Rewards : 9.0\n",
      "Episode 46, Rewards : 128.0\n",
      "Episode 47, Rewards : 9.0\n",
      "Episode 48, Rewards : 10.0\n",
      "Episode 49, Rewards : 70.0\n",
      "Episode 50, Rewards : 8.0\n",
      "Episode 51, Rewards : 24.0\n",
      "Episode 52, Rewards : 8.0\n",
      "Episode 53, Rewards : 9.0\n",
      "Episode 54, Rewards : 10.0\n",
      "Episode 55, Rewards : 11.0\n",
      "Episode 56, Rewards : 10.0\n",
      "Episode 57, Rewards : 11.0\n",
      "Episode 58, Rewards : 79.0\n",
      "Episode 59, Rewards : 84.0\n",
      "Episode 60, Rewards : 414.0\n",
      "Episode 61, Rewards : 8.0\n",
      "Episode 62, Rewards : 10.0\n",
      "Episode 63, Rewards : 152.0\n",
      "Episode 64, Rewards : 43.0\n",
      "Episode 65, Rewards : 8.0\n",
      "Episode 66, Rewards : 26.0\n",
      "Episode 67, Rewards : 20.0\n",
      "Episode 68, Rewards : 85.0\n",
      "Episode 69, Rewards : 10.0\n",
      "Episode 70, Rewards : 20.0\n",
      "Episode 71, Rewards : 9.0\n",
      "Episode 72, Rewards : 31.0\n",
      "Episode 73, Rewards : 9.0\n",
      "Episode 74, Rewards : 77.0\n",
      "Episode 75, Rewards : 25.0\n",
      "Episode 76, Rewards : 55.0\n",
      "Episode 77, Rewards : 73.0\n",
      "Episode 78, Rewards : 108.0\n",
      "Episode 79, Rewards : 160.0\n",
      "Episode 80, Rewards : 24.0\n",
      "Episode 81, Rewards : 11.0\n",
      "Episode 82, Rewards : 9.0\n",
      "Episode 83, Rewards : 9.0\n",
      "Episode 84, Rewards : 26.0\n",
      "Episode 85, Rewards : 92.0\n",
      "Episode 86, Rewards : 10.0\n",
      "Episode 87, Rewards : 9.0\n",
      "Episode 88, Rewards : 59.0\n",
      "Episode 89, Rewards : 8.0\n",
      "Episode 90, Rewards : 34.0\n",
      "Episode 91, Rewards : 80.0\n",
      "Episode 92, Rewards : 9.0\n",
      "Episode 93, Rewards : 10.0\n",
      "Episode 94, Rewards : 111.0\n",
      "Episode 95, Rewards : 278.0\n",
      "Episode 96, Rewards : 9.0\n",
      "Episode 97, Rewards : 10.0\n",
      "Episode 98, Rewards : 8.0\n",
      "Episode 99, Rewards : 10.0\n",
      "Episode 100, Rewards : 8.0\n",
      "Avergae reward: 54.77\n"
     ]
    }
   ],
   "source": [
    "# Learnings in different episodes (different weights of each component of the state to action pairs):\n",
    "n = 100; total_reward=0; best_reward = 0\n",
    "for episode in range(n):\n",
    "    weight = torch.rand(n_state, n_action)\n",
    "    reward = run_episode(env, weight)\n",
    "    total_reward+= reward\n",
    "    print(f'Episode {episode+1}, Rewards : {reward}')\n",
    "    if reward > best_reward:\n",
    "        print(\"\\nBETTER WEIGHTS FOUND !!!\\n\")\n",
    "        best_reward = reward\n",
    "        best_weights = weight\n",
    "\n",
    "print(f'Avergae reward: {total_reward/n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34147ad2-ed23-4c41-a8a2-c3e163b5dc9a",
   "metadata": {},
   "source": [
    "##  *best_weights* has the weights for which the reward was maximum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "8111754c-f592-48d5-a82a-d28c117826bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avergae reward: 293.16\n"
     ]
    }
   ],
   "source": [
    "n = 100; total_reward=0\n",
    "for episode in range(n):\n",
    "    reward = run_episode(env, best_weights)\n",
    "    total_reward+= reward\n",
    "print(f'Avergae reward: {total_reward/n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "2669fd23-09f5-4b9d-9cc3-93b7479f679a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0.02952401, 0.03949724, 0.00313893, 0.00549393], dtype=float32), {})"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "ef9f3114-8371-4363-a9e1-de3d4dccf7e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 0.03574031,  0.04337698, -0.04148225, -0.03843143], dtype=float32),\n",
       " {})"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env.reset()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6553a9f-eaef-407e-84c7-0bad0812d30b",
   "metadata": {},
   "source": [
    "### Why the 'reward' is not same everytime if the weights are same?\n",
    "\n",
    "Given a constant weight, because the starting state is random, the actions can change, and therefore the reward can change. But if the policy is good (the weight for each component), a good weight policy will result in better rewards."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d78e389-7ef0-4f20-8ca5-4172aef6719f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7af524b1-83a8-4d7f-9274-9e7d5fca4dfb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
